{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('selected_col_v2.csv')\n",
    "data = data.drop(['Unnamed: 0'], axis=1)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 features from the most 50th important features of the RF model\n",
    "data_2 = data[['Rank',\n",
    " 'Fruity_flavor',\n",
    " 'COE_score',\n",
    " 'geisha',\n",
    " 'Altitude',\n",
    " 'Citric_acidity',\n",
    " 'Sweet_flavor',\n",
    " 'Floral_flavor',\n",
    " 'NuttyCocoa_flavor',\n",
    " 'Year_2021',\n",
    " 'Country_Nicaragua',\n",
    " 'Tartaric_acidity',\n",
    " 'Farm_santa rosa',\n",
    " 'Year_2020',\n",
    " 'GreenVegetative_flavor',\n",
    " 'Complex_acidity',\n",
    " 'Process_honey',\n",
    " 'Winey_flavor',\n",
    " 'Spices_flavor',\n",
    " 'Malic_acidity',\n",
    " 'Country_Guatemala',\n",
    " 'Process_anaerobic',\n",
    " 'Country_Costa Rica',\n",
    " 'Farm_el cerro',\n",
    " 'typica',\n",
    " 'Lactic_acidity',\n",
    " 'North_America',\n",
    " 'Farm_el paraiso',\n",
    " 'Long_aftertaste',\n",
    " 'Process_natural',\n",
    " 'Country_Ecuador',\n",
    " 'Year_2016',\n",
    " 'pacamara',\n",
    " 'Asia',\n",
    " 'Farm_san luis',\n",
    " 'Creamy_body',\n",
    " 'Country_Ethiopia',\n",
    " 'caturra',\n",
    " 'Process_washed',\n",
    " 'Year_2022',\n",
    " 'Clean_and_clear',\n",
    " 'Country_Brazil',\n",
    " 'Roasted_flavor',\n",
    " 'Farm_platanares',\n",
    " 'Europe',\n",
    " 'Country_Burundi',\n",
    " 'Country_MÃ©xico',\n",
    " 'Country_Honduras',\n",
    " 'Year_2017',\n",
    " 'Country_El Salvador',\n",
    " 'High_bid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (1652, 90)\n",
      "X_subtrain shape =  (1487, 90)\n",
      "X_valid shape =  (165, 90)\n",
      "Y_subtrain shape =  (1487,)\n",
      "Y_valid shape =  (165,)\n",
      "X_test shape =  (414, 90)\n"
     ]
    }
   ],
   "source": [
    "# split training data and test data\n",
    "\n",
    "train, test = train_test_split(data_2, test_size=0.2, random_state=42)\n",
    "\n",
    "# split X y\n",
    "def split_x_y(df):\n",
    "    x = df.drop(['High_bid'], axis=1)\n",
    "    y = df['High_bid']\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = split_x_y(train)\n",
    "x_test, y_test = split_x_y(test)\n",
    "\n",
    "# standardize\n",
    "doscaling = 1\n",
    "if (doscaling == 1):\n",
    "    xscaler = preprocessing.StandardScaler().fit(x_train[['COE_score', 'Altitude']])\n",
    "    # standardize feature values\n",
    "    X_train_conti = xscaler.transform(x_train[['COE_score', 'Altitude']])\n",
    "    X_train = np.concatenate((X_train_conti, x_train.drop(['COE_score', 'Altitude'], axis=1)), axis=1)\n",
    "    X_test_conti = xscaler.transform(x_test[['COE_score', 'Altitude']])\n",
    "    X_test = np.concatenate((X_test_conti, x_test.drop(['COE_score', 'Altitude'], axis=1)), axis=1)\n",
    "else:\n",
    "    X_train = x_train\n",
    "    X_test = x_test\n",
    "\n",
    "\n",
    "y_mean = y_train.mean()\n",
    "Y_train_keep = y_train.copy()\n",
    "Y_test_keep = y_test.copy()\n",
    "\n",
    "Y_train = y_train - y_mean\n",
    "Y_test = y_test - y_mean\n",
    "\n",
    "\n",
    "# validation is the last 10% of training, subtraining is the first 90% of training\n",
    "nvalid = int(X_train.shape[0] * 0.1)\n",
    "nsubtrain = X_train.shape[0] - nvalid\n",
    "\n",
    "X_subtrain = X_train[0:nsubtrain, :].astype('float32')\n",
    "X_valid = X_train[nsubtrain:, :].astype('float32')\n",
    "Y_subtrain = Y_train[0:nsubtrain].astype('float32')\n",
    "Y_valid = Y_train[nsubtrain:].astype('float32')\n",
    "\n",
    "Y_subtrain_keep = Y_train_keep[0:nsubtrain].astype('float32')\n",
    "Y_valid_keep = Y_train_keep[nsubtrain:].astype('float32')\n",
    "\n",
    "print(\"X_train shape = \", X_train.shape)\n",
    "print(\"X_subtrain shape = \", X_subtrain.shape)\n",
    "print(\"X_valid shape = \", X_valid.shape)\n",
    "print(\"Y_subtrain shape = \", Y_subtrain.shape)\n",
    "print(\"Y_valid shape = \", Y_valid.shape)\n",
    "print(\"X_test shape = \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.FloatTensor(X_train)\n",
    "Y_train_t = torch.FloatTensor(Y_train).view(-1, 1)\n",
    "\n",
    "X_subtrain_t = torch.FloatTensor(X_subtrain)\n",
    "Y_subtrain_t = torch.FloatTensor(Y_subtrain).view(-1, 1)\n",
    "\n",
    "X_valid_t = torch.FloatTensor(X_valid)\n",
    "Y_valid_t = torch.FloatTensor(np.array(Y_valid)).view(-1, 1)\n",
    "\n",
    "X_test_t = torch.FloatTensor(X_test)\n",
    "Y_test_t = torch.FloatTensor(np.array(Y_test)).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 500\n",
    "nminibatches = int(np.ceil(X_subtrain.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training step function\n",
    "\n",
    "def output(model, optimizer, loss_f, eval_criterion):\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    best_step_count = 0\n",
    "    \n",
    "    training_rmse_history = []\n",
    "    val_rmse_history = []\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # shuffle the subtraining data for each epoch\n",
    "        shuffle_indices = np.random.permutation(len(X_subtrain_t))\n",
    "        X_subtrain_t_i = X_subtrain_t[shuffle_indices]\n",
    "        Y_subtrain_t_i = Y_subtrain_t[shuffle_indices]\n",
    "\n",
    "        # training step\n",
    "        for step in range(0, X_subtrain_t.shape[0], batch_size):\n",
    "\n",
    "            # minibatch\n",
    "            X_minibatch_t = X_subtrain_t_i[step:step+batch_size]\n",
    "            Y_minibatch_t = Y_subtrain_t_i[step:step+batch_size]\n",
    "\n",
    "            # forward pass\n",
    "            Y_minibatch_pred_t = model(X_minibatch_t)\n",
    "\n",
    "            # compute loss\n",
    "            loss = loss_f(Y_minibatch_pred_t, Y_minibatch_t)\n",
    "\n",
    "            # zero out the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # evaluate the model on the training and validation set every 10 minibatches\n",
    "            if step % (batch_size*100) == 0:\n",
    "\n",
    "                model.eval()\n",
    "                # training set\n",
    "                Y_subtrain_pred_t = model(X_subtrain_t)\n",
    "                training_rmse = torch.sqrt(eval_criterion(Y_subtrain_pred_t, Y_subtrain_t))\n",
    "                training_rmse_history.append(training_rmse.item())\n",
    "\n",
    "                # validation set\n",
    "                Y_valid_pred_t = model(X_valid_t)\n",
    "                val_rmse = torch.sqrt(eval_criterion(Y_valid_pred_t, Y_valid_t))\n",
    "                val_rmse_history.append(val_rmse.item())\n",
    "\n",
    "                # set back to training mode\n",
    "                model.train()\n",
    "\n",
    "                # check if the best model based on val_rmse\n",
    "                if (val_rmse < best_rmse):\n",
    "                    best_rmse = val_rmse\n",
    "                    best_step_count = (epoch_i+1)*nminibatches + step/batch_size\n",
    "                    no_improvement_count = 0\n",
    "\n",
    "                    # save the best model\n",
    "                    best_model = model\n",
    "\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "\n",
    "                # if no improvement for 500 minibatches after best_step_count (another 5 val_rmses), stop training\n",
    "                if (no_improvement_count >= 50):\n",
    "                    print(f'Early stopping at epoch {epoch_i+1}, batch {int(step/batch_size)}.') \n",
    "                    print(f'Best Validation RMSE: {best_rmse:.4f} as best step {int(best_step_count)}.')\n",
    "                    print('')\n",
    "                    break\n",
    "\n",
    "        if (no_improvement_count >= 50):\n",
    "                    break\n",
    "\n",
    "    # the RMSE on the test set of the best model\n",
    "    best_model.eval()\n",
    "    Y_test_pred_t = best_model(X_test_t)\n",
    "    true_list = Y_test_t\n",
    "    pred_list = Y_test_pred_t\n",
    "    test_rmse = torch.sqrt(eval_criterion(Y_test_pred_t, Y_test_t))\n",
    "    print(f'Test RMSE: {test_rmse:.4f}')\n",
    "    \n",
    "    return best_model, best_rmse, best_step_count, test_rmse, training_rmse_history, val_rmse_history, true_list, pred_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the MLP model with 4 hidden layers and dropout\n",
    "class MLPWithDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.layer3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.layer4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.relu1(self.layer1(x)))\n",
    "        x = self.dropout2(self.relu2(self.layer2(x)))\n",
    "        x = self.dropout3(self.relu3(self.layer3(x)))\n",
    "        x = self.dropout4(self.relu4(self.layer4(x)))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 20.3646\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.0001, H = 20\n",
      "----------------------------------------\n",
      "Early stopping at epoch 301, batch 0.\n",
      "Best Validation RMSE: 10.4087 as best step 3765.\n",
      "\n",
      "Test RMSE: 16.2023\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.0005, H = 20\n",
      "----------------------------------------\n",
      "Early stopping at epoch 212, batch 0.\n",
      "Best Validation RMSE: 8.7939 as best step 2430.\n",
      "\n",
      "Test RMSE: 12.9232\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.001, H = 20\n",
      "----------------------------------------\n",
      "Early stopping at epoch 99, batch 0.\n",
      "Best Validation RMSE: 8.9065 as best step 735.\n",
      "\n",
      "Test RMSE: 14.2577\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.005, H = 20\n",
      "----------------------------------------\n",
      "Early stopping at epoch 73, batch 0.\n",
      "Best Validation RMSE: 11.2768 as best step 345.\n",
      "\n",
      "Test RMSE: 16.3383\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.01, H = 20\n",
      "----------------------------------------\n",
      "Early stopping at epoch 80, batch 0.\n",
      "Best Validation RMSE: 12.9604 as best step 450.\n",
      "\n",
      "Test RMSE: 21.8996\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.05, H = 20\n",
      "----------------------------------------\n",
      "Early stopping at epoch 68, batch 0.\n",
      "Best Validation RMSE: 16.0017 as best step 270.\n",
      "\n",
      "Test RMSE: 22.6698\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.1, H = 20\n",
      "----------------------------------------\n",
      "Early stopping at epoch 112, batch 0.\n",
      "Best Validation RMSE: 19.7088 as best step 930.\n",
      "\n",
      "Test RMSE: 24.7129\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.5, H = 20\n",
      "----------------------------------------\n",
      "Early stopping at epoch 338, batch 0.\n",
      "Best Validation RMSE: 19.7220 as best step 4320.\n",
      "\n",
      "Test RMSE: 24.7123\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 1, H = 20\n",
      "----------------------------------------\n",
      "Test RMSE: 14.0474\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.0001, H = 50\n",
      "----------------------------------------\n",
      "Early stopping at epoch 186, batch 0.\n",
      "Best Validation RMSE: 8.5576 as best step 2040.\n",
      "\n",
      "Test RMSE: 11.3089\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.0005, H = 50\n",
      "----------------------------------------\n",
      "Early stopping at epoch 125, batch 0.\n",
      "Best Validation RMSE: 8.2073 as best step 1125.\n",
      "\n",
      "Test RMSE: 11.3564\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.001, H = 50\n",
      "----------------------------------------\n",
      "Early stopping at epoch 74, batch 0.\n",
      "Best Validation RMSE: 8.3103 as best step 360.\n",
      "\n",
      "Test RMSE: 11.8795\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.005, H = 50\n",
      "----------------------------------------\n",
      "Early stopping at epoch 59, batch 0.\n",
      "Best Validation RMSE: 8.1218 as best step 135.\n",
      "\n",
      "Test RMSE: 11.1377\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.01, H = 50\n",
      "----------------------------------------\n",
      "Early stopping at epoch 67, batch 0.\n",
      "Best Validation RMSE: 11.4519 as best step 255.\n",
      "\n",
      "Test RMSE: 22.5478\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.05, H = 50\n",
      "----------------------------------------\n",
      "Early stopping at epoch 85, batch 0.\n",
      "Best Validation RMSE: 16.8780 as best step 525.\n",
      "\n",
      "Test RMSE: 24.7095\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.1, H = 50\n",
      "----------------------------------------\n",
      "Early stopping at epoch 52, batch 0.\n",
      "Best Validation RMSE: 19.6609 as best step 30.\n",
      "\n",
      "Test RMSE: 24.8105\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.5, H = 50\n",
      "----------------------------------------\n",
      "Early stopping at epoch 55, batch 0.\n",
      "Best Validation RMSE: 19.7097 as best step 75.\n",
      "\n",
      "Test RMSE: 26.2308\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 1, H = 50\n",
      "----------------------------------------\n",
      "Early stopping at epoch 425, batch 0.\n",
      "Best Validation RMSE: 8.4200 as best step 5625.\n",
      "\n",
      "Test RMSE: 11.8695\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.0001, H = 100\n",
      "----------------------------------------\n",
      "Early stopping at epoch 159, batch 0.\n",
      "Best Validation RMSE: 8.0093 as best step 1635.\n",
      "\n",
      "Test RMSE: 9.1422\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.0005, H = 100\n",
      "----------------------------------------\n",
      "Early stopping at epoch 78, batch 0.\n",
      "Best Validation RMSE: 8.3415 as best step 420.\n",
      "\n",
      "Test RMSE: 10.5856\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.001, H = 100\n",
      "----------------------------------------\n",
      "Early stopping at epoch 65, batch 0.\n",
      "Best Validation RMSE: 7.5596 as best step 225.\n",
      "\n",
      "Test RMSE: 11.6649\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.005, H = 100\n",
      "----------------------------------------\n",
      "Early stopping at epoch 69, batch 0.\n",
      "Best Validation RMSE: 9.0329 as best step 285.\n",
      "\n",
      "Test RMSE: 15.0803\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.01, H = 100\n",
      "----------------------------------------\n",
      "Early stopping at epoch 54, batch 0.\n",
      "Best Validation RMSE: 12.8474 as best step 60.\n",
      "\n",
      "Test RMSE: 22.4273\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.05, H = 100\n",
      "----------------------------------------\n",
      "Early stopping at epoch 54, batch 0.\n",
      "Best Validation RMSE: 18.0603 as best step 60.\n",
      "\n",
      "Test RMSE: 24.7247\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.1, H = 100\n",
      "----------------------------------------\n",
      "Early stopping at epoch 52, batch 0.\n",
      "Best Validation RMSE: 19.9282 as best step 30.\n",
      "\n",
      "Test RMSE: 24.9080\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.5, H = 100\n",
      "----------------------------------------\n",
      "Early stopping at epoch 56, batch 0.\n",
      "Best Validation RMSE: 20.4382 as best step 90.\n",
      "\n",
      "Test RMSE: 25.1654\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 1, H = 100\n",
      "----------------------------------------\n",
      "Early stopping at epoch 335, batch 0.\n",
      "Best Validation RMSE: 8.7617 as best step 4275.\n",
      "\n",
      "Test RMSE: 11.2586\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.0001, H = 150\n",
      "----------------------------------------\n",
      "Early stopping at epoch 105, batch 0.\n",
      "Best Validation RMSE: 8.0342 as best step 825.\n",
      "\n",
      "Test RMSE: 10.6560\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.0005, H = 150\n",
      "----------------------------------------\n",
      "Early stopping at epoch 78, batch 0.\n",
      "Best Validation RMSE: 7.9209 as best step 420.\n",
      "\n",
      "Test RMSE: 9.3823\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.001, H = 150\n",
      "----------------------------------------\n",
      "Early stopping at epoch 71, batch 0.\n",
      "Best Validation RMSE: 8.3000 as best step 315.\n",
      "\n",
      "Test RMSE: 9.1724\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.005, H = 150\n",
      "----------------------------------------\n",
      "Early stopping at epoch 67, batch 0.\n",
      "Best Validation RMSE: 8.5888 as best step 255.\n",
      "\n",
      "Test RMSE: 11.7092\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.01, H = 150\n",
      "----------------------------------------\n",
      "Early stopping at epoch 54, batch 0.\n",
      "Best Validation RMSE: 14.3189 as best step 60.\n",
      "\n",
      "Test RMSE: 22.7860\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.05, H = 150\n",
      "----------------------------------------\n",
      "Early stopping at epoch 53, batch 0.\n",
      "Best Validation RMSE: 15.9910 as best step 45.\n",
      "\n",
      "Test RMSE: 24.7137\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.1, H = 150\n",
      "----------------------------------------\n",
      "Early stopping at epoch 80, batch 0.\n",
      "Best Validation RMSE: 19.6480 as best step 450.\n",
      "\n",
      "Test RMSE: 25.0351\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.5, H = 150\n",
      "----------------------------------------\n",
      "Early stopping at epoch 85, batch 0.\n",
      "Best Validation RMSE: 20.1037 as best step 525.\n",
      "\n",
      "Test RMSE: 25.3923\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 1, H = 150\n",
      "----------------------------------------\n",
      "Early stopping at epoch 266, batch 0.\n",
      "Best Validation RMSE: 8.9943 as best step 3240.\n",
      "\n",
      "Test RMSE: 11.1709\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.0001, H = 200\n",
      "----------------------------------------\n",
      "Early stopping at epoch 93, batch 0.\n",
      "Best Validation RMSE: 8.2121 as best step 645.\n",
      "\n",
      "Test RMSE: 9.4376\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.0005, H = 200\n",
      "----------------------------------------\n",
      "Early stopping at epoch 74, batch 0.\n",
      "Best Validation RMSE: 7.8940 as best step 360.\n",
      "\n",
      "Test RMSE: 10.7104\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.001, H = 200\n",
      "----------------------------------------\n",
      "Early stopping at epoch 128, batch 0.\n",
      "Best Validation RMSE: 7.3701 as best step 1170.\n",
      "\n",
      "Test RMSE: 10.3442\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.005, H = 200\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 74, batch 0.\n",
      "Best Validation RMSE: 7.2493 as best step 360.\n",
      "\n",
      "Test RMSE: 17.8371\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.01, H = 200\n",
      "----------------------------------------\n",
      "Early stopping at epoch 55, batch 0.\n",
      "Best Validation RMSE: 12.2823 as best step 75.\n",
      "\n",
      "Test RMSE: 24.1819\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.05, H = 200\n",
      "----------------------------------------\n",
      "Early stopping at epoch 57, batch 0.\n",
      "Best Validation RMSE: 19.2832 as best step 105.\n",
      "\n",
      "Test RMSE: 24.7150\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.1, H = 200\n",
      "----------------------------------------\n",
      "Early stopping at epoch 138, batch 0.\n",
      "Best Validation RMSE: 19.6465 as best step 1320.\n",
      "\n",
      "Test RMSE: 25.1178\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 0.5, H = 200\n",
      "----------------------------------------\n",
      "Early stopping at epoch 104, batch 0.\n",
      "Best Validation RMSE: 19.6613 as best step 810.\n",
      "\n",
      "Test RMSE: 29.9616\n",
      "MLP(4 Hidden Layers & Dropout) with Learning Rate = 1, H = 200\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_rmse_list = []\n",
    "learning_rate_list = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "H_list = [20, 50, 100, 150, 200]\n",
    "\n",
    "for h in H_list:\n",
    "\n",
    "    for l in learning_rate_list:\n",
    "\n",
    "        np.random.seed(1321)\n",
    "        model_i = MLPWithDropout(input_size=X_subtrain.shape[1], hidden_size=h)\n",
    "        sse = nn.MSELoss(reduction='sum')\n",
    "        eval_criterion = nn.MSELoss()\n",
    "        optimizer_i = optim.Adam(model_i.parameters(), lr=l)\n",
    "\n",
    "        # training\n",
    "        best_model_i, best_rmse_i, best_step_count_i, test_rmse_i, training_rmse_history_i, val_rmse_history_i, true_list_i, pred_list_i = output(model_i, optimizer_i, sse, eval_criterion)\n",
    "        print(f'MLP(4 Hidden Layers & Dropout) with Learning Rate = {l}, H = {h}')\n",
    "        print('----------------------------------------')\n",
    "        test_rmse_list.append(test_rmse_i.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
